# 概率论基础知识

[TOC]

## 1. 条件概率

条件概率公式: $p(x, y) = p(x) \times p(y | x) = p(y) \times p(x|y)$

如果两个事件是相互独立的, 那么意味着: 无论事件 $x$ 是否发生, 事件 $y$ 发生的概率是不变的, 用公式表示就是: $p(x|y) = p(x)$ , 同理可得: $p(y|x) = p(y)$ 。那么可得: $p(x, y) = p(x) \times p(y)$

如果事件 $x$ 发生导致事件 $y$ 发生的概率降低, 对应 $p(y|x) \lt p(y)$ , 此时相当于 **负相关**, 极端情况下, 如果事件 $x$ 发生, 事件 $y$ 必然不发生, 那么 $p(x, y) = 0$ , 此时两个事件之间就是互斥事件。

如果事件 $x$ 发生导致事件 $y$ 发生的概率升高, 对应 $p(y|x)\gt p(y)$ , 此时相当于 **正相关**, 极端情况下, 如果事件 $x$ 发生, 事件 $y$ 必然发生, 那么 $p(x, y) = p(x)$ , 此时可以说事件 $x$ 包含事件 $y$ 。

有没有可能事件 $x$ **促进** 事件 $y$ 的发生, 但是事件 $y$ **抑制** 事件 $x$ 的发生呢? 答案是不可能, 可以通过反证法得到。

## 2. 概率之间的 "距离"

我们应该如何衡量 **概率** 之间的距离呢?

一般情况下, 我们衡量数字之间的距离用的都是 **减法**, 比方说, 在计算方差时, 我们用减法计算每一个数据点到平均值之间的距离。这种衡量方式对于概率来说, 效果并不好。因为概率的取值范围在 0 到 1 之间, 用减法来衡量的话取值范围就在 -1 到 1 之间。而在实际问题中, 很多事情发生的概率都非常低, 根本没有办法反应出差距。那么应该怎么办呢? 答案是 **除法**。

对于两个概率值 $p_1$ 和 $p_2$ 来说, 如果比值等于 1, 那么 $p_1 = p_2$, 如果大于 1, 那么 $p_1 > p_2$, 如果小于 1, 那么 $p_1 < p_2$ 。

但是这样又会产生问题, 那就是不能像一般的 "距离" 那样进行操作, 更具体的说, 就是不能进行加减运算。比方说, 0.3 和 0.6 之间的距离是 0.5, 但是 0.6 和 0.3 之间的距离是 2。两个距离是有对称性的, 那就是相乘等于 1。但是我们期望的对称性是两者相加等于 0 。那么应该怎么办呢? 答案是取对数。 $\log \frac{p_1}{p_2} = \log p_1 - \log p_2$ 这样就可以很好的解决问题了。

举例来说, 如果事件 A 发生的概率是 $p$, 其不发生的概率是 $1-p$, 它们的比值在英文中有一个专门的词, odds, 即 $odds = \frac{p}{1-p}$ 。对 odds 进行对数操作后, 数学中有一个专门的函数, logit, 即 $\mathbf{logit} = \log (odds)$ 。

logit 是机器学习中的经典函数, 其是 sigmoid 函数的反函数。sigmoid 函数主要用于将模型计算出来的 **分数** 转化成 **概率** 。也就是说, 我们假设模型计算出来的分数就是 logit 值, 然后用 sigmoid 函数将其转换为概率。因此在很多代码中, 模型计算出来的分数变量名记作 logits 。当然, 这样的做法仅限于二分类, 对于多分类问题, 用的是 softmax 函数, 变量名依然沿用 logits 。

总结, 在概率论中, 概率取对数, 以及概率的比值取对数, 都是常规操作。信息论中将概率的负对数定义成 **信息量**, 借此来研究随机变量之间的关系, 这个之后再说。

## 3. 实例: PMI 和 NPMI 指标

我们怎么数值化表示两个事件之间的关联性呢? 一种方式就是使用 **条件概率**, 这个在第一部分已经说过了。另一种做法是利用两个事件的 **联合概率**, 计算 PMI 指标值。

PMI 全称是 Pointwise Mutual Information, 定义如下: 对于事件 $x$ 和 $y$ 来说:

$$
\mathbf{PMI} (x, y) = \log \frac{p(x, y)}{p(x) \cdot p(y)} \tag{1}
$$

理解上一部分所说, 这个公式就很容易理解了。如果事件 $x$ 和事件 $y$ 是相互独立的, 那么 $p(x, y) = p(x) \cdot p(y)$ 。也就是说, PMI 衡量的是 **两个事件的联合概率** 和 **相互对立的情况下, 两个事件的联合概率** 之间的 **"距离"** 。

如果 PMI 小于 0, 意味着两个事件之间是 **相互抑制**, 如果 PMI 值等于 0, 意味着两个事件之间是 **相互独立**, 如果 PMI 值大于 0, 意味着两个事件之间是 **相互促进**。

但是 PMI 是有问题的, 我们知道 $0 \le p(x, y) \le min(p(x), p(y))$ , 也就是说 "相互抑制" 的取值范围和 "相互促进" 的取值范围不是对称的。

怎么办呢? 有人提出了 NPMI (Normalized PMI), 对 PMI 进行标准化操作, 让取值范围在 $[-1, 1]$ 之间, 计算方式如下:

$$
\mathbf{NPMI}(x, y) = \frac{\mathbf{PMI} (x, y)}{- \log p(x, y)} = \frac{\log p(x) \cdot p(y)}{\log p(x, y)} - 1 \tag{2}
$$

分析可以得到 (参考 [stackexchange](https://stats.stackexchange.com/questions/140935) 上的回答):

+ 如果两个事件不会同时发生, 即 $p(x, y) = 0$ , 那么 npmi 的值为 -1
+ 如果两个事件相互独立, 即 $p(x, y) = p(x) \cdot p(y)$ , 那么 npmi 的值为 0
+ 如果两个事件一直同时发生, 即 $p(x, y) = p(x) = p(y)$, 那么 npmi 的值为 1

现在 npmi 是对称的吗? 答案是不是, npmi 只能趋近于 -1, 但是可以取到 1 。数缺形时少直觉, 给出 Python 版本的画图代码:

```python
import warnings
import numpy as np 
import matplotlib.pyplot as plt 

# 预测参数
x_prob = y_prob = 0.4  # x_prob 和 y_prob 的值可以不一样

# 计算值
joint_prob = np.linspace(start=0, stop=min(x_prob, y_prob), num=500, endpoint=False)[1:]  # 去掉 0, 不做平滑处理更容易看出效果
pmi = np.log(joint_prob / (x_prob * y_prob))
npmi = pmi / (-np.log(joint_prob))

# 画图
warnings.filterwarnings(action="ignore")  # noqa 避免 plt 的警告
plt.plot(joint_prob, pmi)
plt.show()

plt.plot(joint_prob, npmi)
plt.show()

plt.plot(joint_prob, npmi ** 2)
plt.show()
```

通过画图, 可以看出, 联合概率和 npmi 之间的关系和 **线性关系** 很像。有时候我们不希望是这样的线性关系, 那么就会对 npmi 进行指数运算, 给更高的联合概率值以更大的权重值, 比方说上面画图中对 npmi 进行的平方操作。

## 4. 随机变量

我们可以认为随机变量是某一些事件的集合, 这些事件之间是 **对立** 关系。比方说, 如果有随机变量是性别, 那么事件有两个: **男** 和 **女**, 此时满足:

+ 两个事件之间是互斥的, 即男事件发生的情况下, 女事件必然不可能发生, 反之亦然。
+ 两个事件之间的概率和为 1。

无论是离散的还是连续的, 都可以这么理解。随机变量的概率分布就是每一种取值的概率值。

需要注意的是, 下面会用小写的 $x$ 表示事件, 大写的 $X$ 表示随机变量, 在学习下面概念时, 一定要注意是针对 **事件** 还是针对 **随机变量** 进行的定义。

## 5. 联合概率, 边缘概率和条件概率

我们知道, 在现实中, 事件之间往往是相互关联的, 那么怎么去研究事件之间的关联性呢? 概率论给出的方法是去研究 **随机变量** 之间的关联性。对于两个随机变量 $X$ 和 $Y$ 而言, 我们定义:

+ 联合概率分布: $P(X, Y)$
+ 条件概率分布: $P(X|Y)$ 和 $P(Y|X)$
+ 边缘概率分布: $P(X)$ 和 $P(Y)$

这五个概率分布之间转换的核心是 **联合概率分布**, 有了它就可以得到其它任意的四个概率分布。主要转化用到两个公式, 条件概率公式和边缘概率公式, 条件概率公式上面已经有了, 这里给出边缘概率公式:

$$
p(x) = \sum_y p(x, y)
$$

给一个具体的例子:

我们往往会根据天气来选择穿什么衣服, 现在有两个随机变量, 一个是天气, 取值有两个: 晴天或者雨天, 一个是衣服, 取值有两个: 短袖或者长袖。

我们假设在下雨天, 穿短袖的概率是 0.25, 穿长袖的概率是 0.75; 在晴天, 穿短袖的概率是 0.75, 穿长袖的概率是 0.25, 那么我们可以得到如下的表格:

|      |   短袖   |   长袖   |
| :----: | :---------: | :---------: |
| 雨天 | $\frac14$ | $\frac43$ |
| 晴天 | $\frac43$ | $\frac14$ |

用公式表示就是 `p(短袖|雨天) = 0.25`, `p(长袖|雨天) = 0.75`, `p(短袖|晴天) = 0.75`, `p(长袖|晴天) = 0.25`

我们假设某地天气的概率分布是: `p(雨天) = 0.25`, `p(晴天) = 0.75`

那么我们就可以计算联合概率了: `p(雨天, 短袖) = p(短袖|雨天) * p(雨天) = 0.25 * 0.25`, 那么可以得到下面这张表格:

|      |      短袖      |      长袖      |
| :----: | :--------------: | :--------------: |
| 雨天 | $\frac{1}{16}$ | $\frac{3}{16}$ |
| 晴天 | $\frac{9}{16}$ | $\frac{3}{16}$ |

通过边缘概率公式, 我们可以知道穿衣服的概率分布: `p(短袖) = p(雨天, 短袖) + p(晴天, 短袖) = 10 / 16`, `p(长袖) = p(雨天, 长袖) + p(晴天, 长袖) = 9 / 16`

再根据条件概率公式, 我们可以通过穿衣服的情况去反推测天气了: `p(雨天|短袖) = p(雨天, 短袖) / p(短袖) = 1 / 10`, 那么可以得到下面这张表格:

|      |      雨天      |      晴天      |
| :----: | :--------------: | :--------------: |
| 短袖 | $\frac{1}{10}$ | $\frac{9}{10}$ |
| 长袖 |   $\frac12$   |   $\frac12$   |

用代码表示如下:

```python
import numpy as np 

# 某地天气的概率
weather_probs = np.array([0.25, 0.75])  # [n_weather]
# 某人穿衣的习惯: 已知天气的情况下, 穿衣服的概率
weather_clothing_probs = np.array([  # [n_weather, n_clothing]
    [0.25, 0.75], 
    [0.75, 0.25]
])

assert np.allclose(weather_clothing_probs.sum(axis=1), 1.0)

# 天气和衣服的联合概率分布
unit_probs = weather_clothing_probs * weather_probs[:, np.newaxis]  # [n_weather, n_clothing]
# 某人的穿衣概率
clothing_probs = unit_probs.sum(axis=0)  # [n_clothing, ]
# 反推概率
clothing_weather_probs = (unit_probs / clothing_probs).T  # [n_clothing, n_weather]

print(unit_probs)
print(clothing_probs)
print(clothing_weather_probs)
```

联合概率本质上是将相互关联的事件转变成了互斥的事件, 因此可以作为枢纽将条件概率和边缘概率联系起来。如果我们想要衡量两个随机事件之间的相关性, 也是通过联合概率分布进行衡量的, 本文后面会解释的。简单来说, 现实世界中事件具有很多关联性, 我们可以把问题转化为 **互斥事件**, 这样问题就变简单了。
