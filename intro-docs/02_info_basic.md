# 信息论基础知识

[TOC]

## 1. 信息量

信息量是针对某一事件来说的, 计算方式是概率的负对数, 即 $h(x) = - \log p(x)$ 。

我们可以将信息量理解成是一种新的概念, 概率和信息量之间的关系可以类比重力和质量之间的关系。如果某一事件发生的概率越低, 那么我们就需要越多的 **信息** 才能确定这一事件的状态。

我们也可以将信息量理解成一种新的空间, 将原本的概率空间转换成信息量空间, 进行运算。

在编码领域, 信息量和比特之间是能建立关系的, 相对而言概念更加清晰一些。

无论在信息论还是机器学习, 概率取对数, 或者概率的比值取对数, 都属于常规操作。因此在这里复习一下 **对数** 的相关知识:

+ $\log(x \times y) = \log x + \log y$
+ $\log \frac{x}{y} = \log x - \log y$
+ $\log x^n = n \times \log x$
+ $(\ln x)^{\prime} = \frac{1}{x}$

根据条件概率公式, 我们可以得到 $h(x, y) = h(x) + h(y|x)$ 。

除此之外, 还需要感性的了解 **对数** 这一概念 (不需要记忆):

+ $\ln 2 \approx 0.69 $
+ $\ln 10^{-15} \approx -34.54 $
+ $\ln 10^{-300} \approx -691$

**重要结论**: 当 $x$ 趋近于 0 时, $\ln x$ 趋近于负无穷, 但是其趋近于负无穷的速度是缓慢的 (虽然其导数/斜率变化很快)。

进一步说明, $x$ 趋近于 0 的速度比 $\ln x$ 趋近于负无穷的速度快很多。或者说: 当 $x$ 趋近于 0 时, $\frac{1}{x}$ 趋近于正无穷的速度远远大于 $-\ln x$ 趋近于正无穷的速度。

用公式表示就是: $\lim \limits_{x \to 0} (x \cdot \ln x) = 0$ , 可以用洛必达法则进行证明, 具体可以参考: [高数技巧 | 七种未定型极限的计算](https://zhuanlan.zhihu.com/p/553569134)

这样, 相比较概率运算, 信息量的运算有如下的优势:

+ 原本乘法的概率运算关系变成了加法
+ 当概率值较小时, 原本趋近于 0 的概率空间变成了趋近于 正无穷 的信息量空间, 同时减缓了趋近的速度。

## 2. 信息熵

熵的本质是描述一个系统混乱度的度量。如何描述一个随机变量的混乱度呢? 一种方式就是信息量的加权平均数, 公式如下:

$$
\begin{equation}
\begin{aligned}
H(X) &= \sum_{i=1}^n p(x_i) \cdot h(x_i) \\
     &= -\sum_{i=1}^n p(x_i) \cdot \log (p(x_i))
\end{aligned}
\end{equation}
\tag{1}
$$

一句话概括就是: 随机变量信息熵等于其值的加权平均数, 而权重就是概率。

用概率对信息量进行加权平均数, 可以看作是一种标准化的过程。

在实际生活中, 过大或者过小的概率对于我们来说都是一个确定的事情, 当 $x$ 趋近于 0 或者 1 时, $x \cdot \log x$ 都趋近于 0 。

比方说抛硬币反面的概率从 0.5 变成 0.1, 对于反面这一事件来说, 不确定性增加了, 对应信息量变大了, 但是对于随机变量来说, 不确定性降低了, 对应信息熵也降低了。

另一方面来说, 信息熵具有对称性, 如果抛硬币反面概率从 0.1 变成 0.9, 对于随机变量来说, 不确定性是不变的, 对应信息熵也是不变的。

那么随机变量的概率分布在什么情况下信息熵可以取到最大值呢? 答案是 **均匀分布**, 感性上很容易理解: 如果随机事件中所有的取值概率都相同, 也就意味着所有事件发生的概率是相同的, 什么都不能确定, 此时不确定性肯定是最高的。

数学证明需要用到拉格朗日数乘法, 可以得到: $H(X) <= \log n$ 。具体可以参考: [关于信息熵最大值的讨论](https://blog.csdn.net/weixin_38468077/article/details/100435303)

这就给我们一个启示: 如果我们不知道随机变量的概率分布, 那么就假设其为均匀分布, 不要进行其它假设, 这样风险最小。在这种情况下, 熵是最大的, 因此被称为 **最大熵模型 (MaxEnt)** 。更具体的:

> 最大熵模型是概率模型学习中一个准则，其思想为：在所有可能的概率模型中，熵最大的模型是最好的。
>
> 若概率模型需要满足一些约束，则最大熵原理就是在满足已知约束的条件集合中选择熵最大模型。
>
> 最大熵原理指出，对一个随机事件的概率分布进行预测时，预测应当满足全部已知的约束，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小，因此得到的概率分布的熵是最大的。

更多关于最大熵模型的可以参考博客: [最大熵模型详解](https://blog.csdn.net/hgnuxc_1993/article/details/114897839) 和 [“熵”不起：从熵、最大熵原理到最大熵模型（二）](https://kexue.fm/archives/3552) 。在约束条件较少的情况下, 可能比较好解决, 但是如果约束条件太多, 就很复杂了。

## 3. 联合熵和条件熵

**联合熵** 描述的是两个随机变量联合概率分布的信息熵, 即:

$$
H(X, Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \cdot h(x, y) \tag{2}
$$

如果理解了信息熵, 那么这里也很容易理解, 只是将单个随机变量变成了两个随机变量的联合。

条件熵是指在给定随机变量 $X$ 的情况下, 随机变量 $Y$ 的条件概率分布的熵的期望, 数学公式如下:

$$
\begin{equation}
\begin{aligned}
H(Y|X) &= \sum_{x \in X} p(x) \cdot H(Y|x) \\
       &= \sum_{x \in X} p(x) \cdot \sum_{y \in Y} p(y|x) \cdot h(y|x)\\
       &= \sum_{x \in X} \sum_{y \in Y} p(x, y) \cdot h(y|x)
\end{aligned}
\end{equation}
\tag{3}
$$

特别需要提示的是, 在条件熵中, $X$ 也是变量, 不是一个固定的数, 这一点很容易出错。

那么联合熵和条件熵之间有什么关系呢?

之前说过, 随机变量的概率分布和为 1, 也就是说:

$$
p(x) = \sum_{y \in Y} p(x, y) \tag{4}
$$

我们将公式 $(4)$ 代入公式 $(1)$ 中, 可以得到:

$$
\begin{equation}
\begin{aligned}
H(X) &= \sum_{x \in X} p(x) \cdot h(x) \\
     &= \sum_{x \in X} \sum_{y \in Y} p(x, y) \cdot h(x)
\end{aligned}
\end{equation}
\tag{5}
$$

观察公式 $(2)$, $(3)$ 和 $(5)$, 根据根据公式: $h(x, y) = h(y|x) + h(x)$ , 我们可得:

$$
\begin{equation}
\begin{aligned}
H(X, Y) &= H(X) + H(Y|X) \\
        &= H(Y) + H(X|Y)
\end{aligned}
\end{equation}
\tag{6}
$$

显然, 如果随机变量 $X$ 和 $Y$ 中任意两个事件之间都是独立事件, 那么 $H(X, Y) = H(X) + H(Y)$ 。

从这里可以看出, 条件熵和条件概率在公式上保持着高度的相似性。

## 4. 互信息

**互信息** 也被称为 **信息增益**, 指的是对于随机事件 $X$ 而言, 知道和不知道随机事件 $Y$ 的情况下, 信息熵的差值, 用公式表示为:

$$
\begin{equation}
\begin{aligned}
I(X;Y) &= H(X) - H(X|Y) \\
       &= \sum_{x \in X} \sum_{y \in Y} p(x, y) \cdot [ h(x) + h(y) - h(x, y)]
\end{aligned}
\end{equation}
\tag{7}
$$

从上式可以看出, 互信息具有 **对称性**, 即: $I(X;Y) = I(Y;X)$, 同时可以通过证明得到 **非负性**。

将公式 $(7)$ 进行变换, 可以得到:

$$
\begin{equation}
\begin{aligned}
I(X;Y) &= H(X) + H(Y) - H(X, Y) \\
       &= H(X, Y) - H(X|Y) - H(Y|X)
\end{aligned}
\end{equation}
\tag{8}
$$

网上很多教程会将公式 $(8)$ 用韦恩图的形式表示出来, 具体参考: [联合熵、条件熵、互信息、相对熵、交叉熵的详解](https://blog.csdn.net/ding_programmer/article/details/90140643) , 这里说明一下图的理解方式:

+ 概率的韦恩图是针对事件而言的, 相交表示两个事件同时发生, 不能表现出两个事件之间是否独立
+ 信息熵的韦恩图是针对随机变量而言的, 相交表示两个变量的相关性, 不能表现 "同时发生" (随机变量中也没有这一概念)

如果想可视化理解, 可以参考博客 [Visual Information Theory](http://colah.github.io/posts/2015-09-Visual-Information/) 。

额外说一点, 为什么互信息中用的是分号, 按照正常理解, 分号左右应该表示自变量和参数, 这里应该用逗号才对。这里确实应该用逗号, 用分号是为了书写好看, 一种约定俗称的用法, 具体参考: [What does the semicolon mean in mutual information?](https://math.stackexchange.com/questions/3820274/what-does-the-semicolon-mean-in-ixy-mutual-information) 。

## 5. KL 散度

信息熵是一个对称的系统, 随机变量的概率分布在 $[0.1, 0.4, 0.5]$ 和 $[0.1, 0.5, 0.4]$ 时信息熵是相同的, 这符合我们对于信息熵的理解 (随机变量的不确定性没有发生变化)。但是当我们衡量两个分布之间的距离时, 这样就会出现问题, 不同的概率分布计算出来的距离是 0 。

那么怎么解决呢? 答案是将随机变量中的每一个事件都计算信息量的差值, 然后再进行标准化。用什么权重进行标准化呢? 以谁为基准就用谁的概率作为权重进行标准化(在数学中, **基准** 往往都是除数或者减数)。这样就可以得到 KL 散度的公式了:

$$
KL(P||Q) = \sum_{i=1}^n p(x_i) \cdot [h(q(x_i)) - h(p(x_i))] \tag{9}
$$

特别的, 如果 $H(P) = 0$, 那么公式 $(9)$ 就变成了:

$$
CE(P, Q) = \sum_{i=1}^n p(x_i) \cdot h(q(x_i)) \tag{10}
$$

我们将公式 $(10)$ 称为交叉熵。从这里可以看出, 交叉熵和 KL 散度在本质上是一样的, 都是衡量两个概率分布之间的差异。那么什么情况下 $H(P)$ 的值为 0 呢?

在逻辑回归 / softmax 回归中, 一般情况下, $P$ 分布是 **真实的标签分布**, $Q$ 分布是 **模型预测的标签分布**, 我们希望 $Q$ 分布靠近 $P$ 分布。

我们只知道样本属于哪一个标签, 怎么得到所有标签的概率分布呢? 答案是用最暴力的 one-hot 方式, 即样本属于的那一个标签概率是 1, 其它标签的概率值是 0。

这个时候就会出现问题, `log(0)` 是没有定义的, 怎么办呢? 我们特别定义: $0 \cdot \log 0 = 0$ 。可以这么定义的原因在第一部分已经说明了。显然这个时候, **真实的标签概率分布** 所对应的信息熵为 0。这也符合我们对于信息熵的理解: 确定的事情不确定性应该为 0。

这样, 我们就可以用交叉熵作为分类任务的损失函数。KL 散度也经常作为深度学习任务的损失函数, 比方说蒸馏模型。

用一句话概括交叉熵就是: 用某一概率分布计算信息量, 用另一概率分布作为权重值, 计算加权平均数/期望。

当且仅当 $P$ 分布和 $Q$ 分布一致时, $CE(P, Q)$ 取得最小值, 最小值为 $H(P)$ 。证明方式可以查询 **琴生不等式** (Gibbs' inequality)。这里也说明了交叉熵的 **非负性** 。

同样地, KL 散度具有非负性, 因此我们不需要像 L1 距离和 L2 距离那样, 将差值取绝对值或者平方。但是也有问题, 那就是标准化。由于我们使用了 基准 的概率分布作为权重值, 这就破环了距离的对称性, 即 $KL(P||Q) \neq KL(Q||P)$ , 同时 KL 散度也不满足三角不等式, 即 $KL(P||Q)$ 有可能大于 $KL(P||R) + KL(R||Q)$ 的值。

那么怎么解决不对成和不满足三角不等式的问题呢? 答案是 JS 散度。

我们定义 $M$ 分布, 其是 $P$ 分布和 $Q$ 分布的平均数, 即 $M=\frac{P+Q}{2}$, 那么 JS 散度的定义如下:

$$
JS(P||Q) = \frac12 \cdot KL(P||M) + \frac12 \cdot KL(Q||M) \tag{11}
$$

即分别以 $P$ 分布和 $Q$ 为基准, 计算 $M$ 分布到两者的 KL 散度, 再取平均值。当然, 这样也是有问题的, 更多相关知识可以参考: [理解JS散度(Jensen–Shannon divergence)](https://blog.csdn.net/weixin_44441131/article/details/105878383) 和 [机器学习中的数学——距离定义：基础知识](https://blog.csdn.net/hy592070616/article/details/121723169) 系列博客, 这里就不继续挖坑了。

最后再说一点, 那就是 KL 散度和互信息之间是有关系的, 我们设 $h^{\prime}(x, y)$ 表示事件 $x$ 和事件 $y$ 相互独立时的联合概率信息量, 那么可以得到: $h^\prime(x, y) = h(x) + h(y)$, 观察互信息的公式 $(7)$, 不难得出:

$$
\begin{equation}
\begin{aligned}
I(X;Y) &= \sum_{x \in X} \sum_{y \in Y} p(x, y) \cdot [ h(x) + h(y) - h(x, y)] \\
       &= \sum_{x \in X} \sum_{y \in Y} p(x, y) \cdot [ h^\prime(x, y) - h(x, y)]
\end{aligned}
\end{equation}
\tag{12}
$$

结合 KL 散度公式, 我们可以得出: $I(X;Y) = KL(A||B)$ , 其中, $A$ 表示 $X$ 和 $Y$ 的联合概率分布, $B$ 表示 $X$ 和 $Y$ 在随机变量相互独立情况下的联合概率分布, 进一步表明互信息的含义。

从这里可以看出, 互信息和 PMI 之间的关系。都是用 **联合概率** 和 **相互独立时的联合概率** 的比值对数来衡量两个事件之间的相关性。区别在于互信息是针对 **随机变量** 定义的, 有标准化的操作, 而 PMI 是针对 **事件** 定义的, 标准化是 npmi 。

额外说明一下, 数学符号 $||$ 主要表示散度, 常常用于和散度相关的公式中 (但也不绝对), 散度又是一个大坑, 这里就不展开了。

## References

+ [如何通俗的解释交叉熵与相对熵？](https://www.zhihu.com/question/41252833/answer/2347817584)
+ [机器学习丨什么是互信息](https://www.cnblogs.com/vincent1997/articles/12290841.html)
+ [通俗理解条件熵](https://zhuanlan.zhihu.com/p/26551798)
